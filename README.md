# ğŸš€ High-Performance Planning with JAX, CUDA & Differentiable Simulation

## ğŸ“Œ Overview
This project focuses on **high-speed path planning** using:
- **JAX for acceleration** (JIT-compilation & vectorization)
- **CUDA for parallel A*** on GPU
- **Differentiable Simulation** for gradient-based refinement
- **LLMs for heuristic enhancement** (GPT-4 guided heuristics)
- **RL for adaptive planning** (PPO-trained policies)

Additionally, we analyze **scaling laws** in:
- **Compute Efficiency** (JAX/CUDA speedup)
- **Model Scaling** (GPT-3.5 vs. GPT-4 heuristics)
- **Data Efficiency** (RL performance with increasing samples)

---

## âœ¨ Features
âœ… **Baseline A\*** (NumPy-based)  
âœ… **JAX-Accelerated A\*** (JIT-compiled for speedup)  
âœ… **CUDA-Parallelized A\*** (GPU-based acceleration)  
âœ… **Differentiable Path Optimization** (Gradient-based refinement)  
âœ… **LLM-Guided Heuristics** (GPT-4 for heuristic improvement)  
âœ… **RL-Based Adaptive Planning** (PPO-learned heuristics)  
âœ… **Scaling Analysis** (Compute, model size, and data efficiency)  

---

## ğŸ“ Project Structure
```bash
ğŸ“‚ high-performance-planning/
â”œâ”€â”€ ğŸ“œ README.md          # Project Documentation
â”œâ”€â”€ ğŸ“œ requirements.txt   # Dependencies
â”œâ”€â”€ ğŸ“ experiments/       # Benchmarking results & scaling analysis
â”œâ”€â”€ ğŸ—ï¸ src/               # Implementation files
â”‚   â”œâ”€â”€ a_star.py        # Baseline A* planner
â”‚   â”œâ”€â”€ jax_a_star.py    # JAX-accelerated A* planner
â”‚   â”œâ”€â”€ cuda_a_star.cu   # CUDA-optimized A* planner
â”‚   â”œâ”€â”€ differentiable_planner.py  # Differentiable path optimization
â”‚   â”œâ”€â”€ llm_heuristics.py # LLM-based heuristic integration
â”‚   â”œâ”€â”€ rl_planner.py    # RL-based heuristic learning
â”‚   â”œâ”€â”€ utils.py         # Helper functions
â”‚   â””â”€â”€ visualize.py     # Visualization tools
â””â”€â”€ ğŸ“œ benchmarks/       # Performance comparison results
